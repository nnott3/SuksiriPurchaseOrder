{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a5bc65a",
   "metadata": {},
   "source": [
    "# Convert PDF to JPG for faster parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f22d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pdf2image import convert_from_path\n",
    "\n",
    "# Input & output folders\n",
    "INPUT_DIR = \"pdf_folder\"\n",
    "OUTPUT_DIR = \"jpg_output\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Loop through all PDF files in folder\n",
    "for file in os.listdir(INPUT_DIR):\n",
    "    if file.lower().endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(INPUT_DIR, file)\n",
    "        base_name = os.path.splitext(file)[0]\n",
    "\n",
    "        # Convert PDF to images\n",
    "        images = convert_from_path(pdf_path)\n",
    "\n",
    "        # Save each page as JPG with page index\n",
    "        for i, img in enumerate(images, start=1):\n",
    "            out_file = f\"{base_name}_page{i}.jpg\"\n",
    "            img.save(os.path.join(OUTPUT_DIR, out_file), \"JPEG\")\n",
    "\n",
    "        print(f\"✅ Saved {len(images)} pages from {file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002c6a54",
   "metadata": {},
   "source": [
    "# Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75969e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "import gspread\n",
    "from agentic_doc.parse import parse\n",
    "from pydantic import BaseModel, Field\n",
    "from agentic_doc.connectors import LocalConnectorConfig\n",
    "from google.oauth2.service_account import Credentials\n",
    "from googleapiclient.discovery import build\n",
    "\n",
    "def get_google_credentials():\n",
    "    \"\"\"Get Google credentials from Streamlit secrets or local file.\"\"\"\n",
    "    cred_path = \"suksiri-purchase-test-0f09e84df6dd.json\"\n",
    "    if os.path.exists(cred_path):\n",
    "        return Credentials.from_service_account_file(\n",
    "            cred_path,\n",
    "            scopes=[\"https://www.googleapis.com/auth/spreadsheets\"]\n",
    "        )\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Credentials file not found: {cred_path}\")\n",
    "\n",
    "def get_spreadsheet_id():\n",
    "    \"\"\"Get spreadsheet ID from secrets or use default.\"\"\"\n",
    "    return \"17chQLsKcpyZNnJyw8Ads-WRz45kNvI1AbvwsdIlcXqs\"\n",
    "\n",
    "\n",
    "############################### Store_list ##############################\n",
    "def get_store_list():\n",
    "    \"\"\"Fetch and process store list from Google Sheets.\"\"\"\n",
    "\n",
    "    # Get credentials and spreadsheet ID\n",
    "    creds = get_google_credentials()\n",
    "    spreadsheet_id = get_spreadsheet_id()\n",
    "    \n",
    "    client = gspread.authorize(creds)\n",
    "    service = build(\"sheets\", \"v4\", credentials=creds)\n",
    "\n",
    "    sheets = client.open_by_key(spreadsheet_id)\n",
    "    store_data = sheets.worksheet(\"ข้อมูลร้านค้า\")\n",
    "    sheet = sheets.worksheet(\"รายการสินค้า\")\n",
    "\n",
    "    table_range = \"ข้อมูลร้านค้า!B2:G\"  \n",
    "\n",
    "    result = service.spreadsheets().values().get(\n",
    "        spreadsheetId=spreadsheet_id,\n",
    "        range=table_range\n",
    "    ).execute()\n",
    "\n",
    "    values = result.get(\"values\", [])\n",
    "    max_len = len(values[0])\n",
    "    normalized = [row + [\"\"] * (max_len - len(row)) for row in values[1:]]\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df_table2 = pd.DataFrame(normalized, columns=values[0])  # first row is header\n",
    "\n",
    "    # --- Keep only rows where 'ร้านค้า' is not None or empty ---\n",
    "    df_filtered = df_table2[df_table2['ร้านค้า'].notna() & (df_table2['ร้านค้า'] != '')].copy()\n",
    "\n",
    "    # --- Convert 'ยังไม่รวม VAT' from string 'TRUE'/'FALSE' to boolean ---\n",
    "    df_filtered['ยังไม่รวม VAT'] = df_filtered['ยังไม่รวม VAT'].map(lambda x: True if str(x).upper() == 'TRUE' else False)\n",
    "\n",
    "    unique_store_list = df_filtered['ร้านค้า'].unique().tolist()\n",
    "\n",
    "    return unique_store_list, df_filtered\n",
    "    \n",
    " \n",
    "############################### Build_Rows ##############################\n",
    "def build_rows(fields):\n",
    "    metadata = {\n",
    "        \"วันเดือนปี\": fields.documentInfo.documentDate,\n",
    "        \"ร้านค้า\": fields.companyInfo.companyName,\n",
    "        \"เลขกำกับ\": fields.documentInfo.documentNumber,\n",
    "        # \"taxId\": fields.companyInfo.taxId,\n",
    "        # \"customerName\": fields.customerInfo.customerName,\n",
    "        # \"grossAmount\": fields.totals.grossAmount,\n",
    "        # \"netAmount\": fields.totals.netAmount,\n",
    "        # \"vat\": fields.totals.vat,\n",
    "        # \"grandTotal\": fields.totals.grandTotal,\n",
    "    }\n",
    "    rows = []\n",
    "    # Handle items\n",
    "    for i, item in enumerate(fields.items):\n",
    "        row = metadata.copy()\n",
    "        row[\"รายการสินค้า\"] = item.description\n",
    "        row[\"จำนวน\"] = float(item.quantity.replace(\",\", \"\"))\n",
    "        row[\"หน่วย\"] = item.unitName\n",
    "        row[\"ราคาต่อหน่วย\"] = float(item.unitPrice.replace(\",\", \"\"))\n",
    "        \n",
    "        # set defaults to empty cells\n",
    "        row[\"ลดราคา(%)\"], row[\"ลดราคา(บาท)\"] = \"\", \"\"\n",
    "        if item.discountType == \"บาท\":\n",
    "            row[\"ลดราคา(บาท)\"] = float(item.discount.strip('฿').replace(\",\", \"\"))\n",
    "        elif item.discountType == \"Percentage\":\n",
    "            row[\"ลดราคา(%)\"] = float(item.discount.strip('%'))\n",
    "\n",
    "        rows.append(row)\n",
    "    \n",
    "    return rows\n",
    "\n",
    "############################### update_sheet ##############################\n",
    "def update_sheet(rows):\n",
    "    \n",
    "    # Get credentials and spreadsheet ID\n",
    "    creds = get_google_credentials()\n",
    "    spreadsheet_id = get_spreadsheet_id()\n",
    "    client = gspread.authorize(creds)\n",
    "    service = build(\"sheets\", \"v4\", credentials=creds)\n",
    "\n",
    "    sheets = client.open_by_key(spreadsheet_id)\n",
    "    store_data = sheets.worksheet(\"ข้อมูลร้านค้า\")\n",
    "    sheet = sheets.worksheet(\"รายการสินค้า\")\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    values = df.values.tolist()\n",
    "    \n",
    "    start_row = len(sheet.get_all_values()) + 1  # +1 because Sheets are 1-indexed\n",
    "    for i, row in enumerate(values):\n",
    "        current_row = start_row + i\n",
    "\n",
    "        # Column J: ยอดเงิน\n",
    "        row.append(\"=Transactions_2[จำนวน]*Transactions_2[ราคาต่อหน่วย]\")\n",
    "\n",
    "        # Column K: ยอดเงินหลังลดราคา\n",
    "        row.append(\"=IF(Transactions_2[ลดราคา(%)], Transactions_2[ยอดเงิน]*(1-Transactions_2[ลดราคา(%)]/100), Transactions_2[ยอดเงิน]-Transactions_2[ลดราคา(บาท)])\")\n",
    "\n",
    "        # Column L: ยอดรวมต่อรายการ\n",
    "        row.append(f\"=SUMIF(Transactions_2[เลขกำกับ], C{current_row}, Transactions_2[ยอดเงินหลังลดราคา])\")\n",
    "\n",
    "        # Column M: ยอดรวมหลังภาษี\n",
    "        row.append(\n",
    "            f\"=IF(XLOOKUP(B{current_row}, Table2[ร้านค้า], Table2[ยังไม่รวม VAT], FALSE), $L{current_row}*1.07, $L{current_row})\"\n",
    "        )\n",
    "\n",
    "    # Append all rows to the sheet\n",
    "    sheet.append_rows(values, value_input_option=\"USER_ENTERED\")\n",
    "\n",
    "    return pd.DataFrame(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e47455",
   "metadata": {},
   "source": [
    "# ModelClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee5bf1c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_cache is only supported with oauth2client<4.0.0 (__init__.py:49)\n"
     ]
    }
   ],
   "source": [
    "unique_store_list, _ = get_store_list()\n",
    "####################### ExtractedDocumentFieldsSchema Class ##############################\n",
    "class CompanyInfo(BaseModel):\n",
    "    companyName: str = Field(\n",
    "        ...,\n",
    "        description=f'Identify the official name of the seller/supplier company as stated in the document. Compare it to the following list of known companies: [{\", \".join(unique_store_list)}]. Return the closest match, ignoring common words like [\"ห้างหุ้นส่วนจำกัด\", \"บริษัท\", \"จำกัด\", \"บจก.\", \"หจก.\", \"จํากัด\", \"ก้าวไกล\"] for matching purposes. If no sufficiently similar match is found, retain the original text from the document.',\n",
    "        title='Company Name',\n",
    "    )\n",
    "    taxId: str = Field(\n",
    "        ...,\n",
    "        description=\"The seller/supplier company's tax identification number.\",\n",
    "        title='Tax Identification Number',\n",
    "    )\n",
    "\n",
    "class CustomerInfo(BaseModel):\n",
    "    customerName: str = Field(\n",
    "        ..., description='The name of the customer or recipient.', title='Customer Name'\n",
    "    )\n",
    "    \n",
    "class DocumentInfo(BaseModel):\n",
    "    documentNumber: str = Field(\n",
    "        ...,\n",
    "        description='Unique identifier or reference number(เลขที่กำกับ) for the document.',\n",
    "        title='Document Number',\n",
    "    )\n",
    "    documentDate: str = Field(\n",
    "        ..., description='Date the document was issued with Year formatted in คริสต์ศักราช (ค.ศ.)/AD (Anno Domini) if it was initially written in the format of พุทธศักราช (พ.ศ.)/BE (Buddhist Era)', title='Document Date'\n",
    "    )\n",
    "    \n",
    "class Item(BaseModel):\n",
    "    description: str = Field(\n",
    "        ..., description='Description of the item or service.', title='Description'\n",
    "    )\n",
    "    quantity: str = Field(..., description='Quantity of the item.', title='Quantity')\n",
    "    unitPrice: str = Field(\n",
    "        ..., description='Price per unit of the item.', title='Unit Price'\n",
    "    )\n",
    "    unitName: str = Field(..., description='Unit of measurement for the item, for example, meter, Pcs, ea, kg, box, อัน, ใบ, เส้น, ท่อน, ตัว, กระป๋อง, หลอด, ม้วน', title='Unit Name')\n",
    "    amount: str = Field(..., description='Total amount for the item.', title='Amount')\n",
    "    discount: str = Field(..., description='Discount applied to the item.', title='Item Discount')\n",
    "    discountType: str = Field(..., description='Type of discount applied in Thai Baht or in Percentage or No Discount', title='Discount Type')\n",
    "\n",
    "class Totals(BaseModel):\n",
    "    grossAmount: str = Field(\n",
    "        ...,\n",
    "        description='Total gross amount before discounts and taxes.',\n",
    "        title='Gross Amount',\n",
    "    )\n",
    "    netAmount: str = Field(\n",
    "        ..., description='Net amount after discounts.', title='Net Amount'\n",
    "    )\n",
    "    vat: str = Field(..., description='Value-added tax amount.', title='VAT')\n",
    "    grandTotal: str = Field(\n",
    "        ...,\n",
    "        description='Total amount payable including all taxes.',\n",
    "        title='Grand Total',\n",
    "    )\n",
    "\n",
    "class ExtractedDocumentFieldsSchema(BaseModel):\n",
    "    \n",
    "    companyInfo: CompanyInfo = Field(\n",
    "        ...,\n",
    "        description='Key company details from headers and form fields.',\n",
    "        title='Company Information',\n",
    "    )\n",
    "    customerInfo: CustomerInfo = Field(\n",
    "        ...,\n",
    "        description='Details about the customer or recipient of the invoice/delivery.',\n",
    "        title='Customer Information',\n",
    "    )\n",
    "    documentInfo: DocumentInfo = Field(\n",
    "        ...,\n",
    "        description='Key identifiers and dates for the document.',\n",
    "        title='Document Information',\n",
    "    )\n",
    "    items: List[Item] = Field(\n",
    "        ...,\n",
    "        description='List of items, products, or services from the main table(s) in the document.',\n",
    "        title='Itemized Table',\n",
    "    )\n",
    "    totals: Totals = Field(\n",
    "        ...,\n",
    "        description='Summary of financial totals from the document.',\n",
    "        title='Totals and Summary',\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4889f5e",
   "metadata": {},
   "source": [
    "# Main Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c60cfaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "drive-download-20250912T082149Z-1-001/8สค68\n",
      "Files with *.jpg...\n",
      "\u001b[2m2025-09-16 14:47:40\u001b[0m [info   \u001b[0m] \u001b[1mAPI key is valid.             \u001b[0m [\u001b[0m\u001b[1m\u001b[34magentic_doc.utils\u001b[0m]\u001b[0m (utils.py:42)\n",
      "\u001b[2m2025-09-16 14:47:40\u001b[0m [info   \u001b[0m] \u001b[1mParsing 9 documents           \u001b[0m [\u001b[0m\u001b[1m\u001b[34magentic_doc.parse\u001b[0m]\u001b[0m (parse.py:280)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents:   0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.va.landing.ai/v1/tools/agentic-document-analysis \"HTTP/1.1 200 OK\" (_client.py:1025)\n",
      "\u001b[2m2025-09-16 14:47:59\u001b[0m [info   \u001b[0m] \u001b[1mTime taken to successfully parse a document chunk: 19.37 seconds\u001b[0m [\u001b[0m\u001b[1m\u001b[34magentic_doc.parse\u001b[0m]\u001b[0m (parse.py:827)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents:  11%|█         | 1/9 [00:19<02:34, 19.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.va.landing.ai/v1/tools/agentic-document-analysis \"HTTP/1.1 200 OK\" (_client.py:1025)\n",
      "\u001b[2m2025-09-16 14:48:13\u001b[0m [info   \u001b[0m] \u001b[1mTime taken to successfully parse a document chunk: 33.38 seconds\u001b[0m [\u001b[0m\u001b[1m\u001b[34magentic_doc.parse\u001b[0m]\u001b[0m (parse.py:827)\n",
      "HTTP Request: POST https://api.va.landing.ai/v1/tools/agentic-document-analysis \"HTTP/1.1 200 OK\" (_client.py:1025)\n",
      "\u001b[2m2025-09-16 14:48:19\u001b[0m [info   \u001b[0m] \u001b[1mTime taken to successfully parse a document chunk: 38.98 seconds\u001b[0m [\u001b[0m\u001b[1m\u001b[34magentic_doc.parse\u001b[0m]\u001b[0m (parse.py:827)\n",
      "HTTP Request: POST https://api.va.landing.ai/v1/tools/agentic-document-analysis \"HTTP/1.1 200 OK\" (_client.py:1025)\n",
      "\u001b[2m2025-09-16 14:48:19\u001b[0m [info   \u001b[0m] \u001b[1mTime taken to successfully parse a document chunk: 39.07 seconds\u001b[0m [\u001b[0m\u001b[1m\u001b[34magentic_doc.parse\u001b[0m]\u001b[0m (parse.py:827)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents:  22%|██▏       | 2/9 [00:39<02:16, 19.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.va.landing.ai/v1/tools/agentic-document-analysis \"HTTP/1.1 200 OK\" (_client.py:1025)\n",
      "\u001b[2m2025-09-16 14:48:22\u001b[0m [info   \u001b[0m] \u001b[1mTime taken to successfully parse a document chunk: 22.77 seconds\u001b[0m [\u001b[0m\u001b[1m\u001b[34magentic_doc.parse\u001b[0m]\u001b[0m (parse.py:827)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents:  56%|█████▌    | 5/9 [00:42<00:25,  6.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.va.landing.ai/v1/tools/agentic-document-analysis \"HTTP/1.1 200 OK\" (_client.py:1025)\n",
      "\u001b[2m2025-09-16 14:48:49\u001b[0m [info   \u001b[0m] \u001b[1mTime taken to successfully parse a document chunk: 30.25 seconds\u001b[0m [\u001b[0m\u001b[1m\u001b[34magentic_doc.parse\u001b[0m]\u001b[0m (parse.py:827)\n",
      "HTTP Request: POST https://api.va.landing.ai/v1/tools/agentic-document-analysis \"HTTP/1.1 200 OK\" (_client.py:1025)\n",
      "\u001b[2m2025-09-16 14:48:53\u001b[0m [info   \u001b[0m] \u001b[1mTime taken to successfully parse a document chunk: 39.66 seconds\u001b[0m [\u001b[0m\u001b[1m\u001b[34magentic_doc.parse\u001b[0m]\u001b[0m (parse.py:827)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents:  67%|██████▋   | 6/9 [01:13<00:37, 12.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.va.landing.ai/v1/tools/agentic-document-analysis \"HTTP/1.1 200 OK\" (_client.py:1025)\n",
      "\u001b[2m2025-09-16 14:49:04\u001b[0m [info   \u001b[0m] \u001b[1mTime taken to successfully parse a document chunk: 44.43 seconds\u001b[0m [\u001b[0m\u001b[1m\u001b[34magentic_doc.parse\u001b[0m]\u001b[0m (parse.py:827)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents:  89%|████████▉ | 8/9 [01:23<00:09,  9.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HTTP Request: POST https://api.va.landing.ai/v1/tools/agentic-document-analysis \"HTTP/1.1 200 OK\" (_client.py:1025)\n",
      "\u001b[2m2025-09-16 14:49:06\u001b[0m [info   \u001b[0m] \u001b[1mTime taken to successfully parse a document chunk: 44.05 seconds\u001b[0m [\u001b[0m\u001b[1m\u001b[34magentic_doc.parse\u001b[0m]\u001b[0m (parse.py:827)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing documents: 100%|██████████| 9/9 [01:26<00:00,  9.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "Saved drive-download-20250912T082149Z-1-001/8สค68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configuration\n",
    "BASE_DIR = \"drive-download-20250912T082149Z-1-001\"\n",
    "SUPPORTED_EXTENSIONS = [\"*.jpg\"]\n",
    "OUTPUT_DIR = \"batch_processing_results\"\n",
    "\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Find directories with files\n",
    "subdirs = []\n",
    "for root, dirs, files in os.walk(BASE_DIR):\n",
    "    if any(any(file.lower().endswith(ext.replace(\"*\", \"\").lower()) for ext in SUPPORTED_EXTENSIONS) for file in files):\n",
    "        subdirs.append(root)\n",
    "\n",
    "all_rows = []\n",
    "\n",
    "# Process each directory\n",
    "for subdir in sorted(subdirs)[:1]:\n",
    "    print('='*60)\n",
    "    print(subdir)\n",
    "    \n",
    "    config = LocalConnectorConfig()\n",
    "    \n",
    "    \n",
    "    \n",
    "    for ext in SUPPORTED_EXTENSIONS:\n",
    "        all_rows = []\n",
    "        \n",
    "        print(f\"Files with {ext}...\")\n",
    "        extraction_results = parse(\n",
    "            config,\n",
    "            connector_path=subdir,\n",
    "            connector_pattern=ext,\n",
    "            extraction_model=ExtractedDocumentFieldsSchema\n",
    "        )\n",
    "        \n",
    "        for result in extraction_results:\n",
    "            try:\n",
    "                fields = result.extraction\n",
    "                rows = build_rows(fields)\n",
    "                all_rows.extend(rows)  # extend instead of append\n",
    "            except Exception:\n",
    "                continue\n",
    "                \n",
    "\n",
    "\n",
    "        df = pd.DataFrame(all_rows)\n",
    "\n",
    "        if df.empty:\n",
    "            print('df.empty')\n",
    "        else:\n",
    "            # Save to CSV\n",
    "            CSV_OUTPUT = f\"{subdir.split('/')[-1]}_{ext.split('*.')[-1]}_extracted_data_{datetime.now().strftime('%Y%m%d%H%M%S')}.csv\"\n",
    "            csv_path = os.path.join(OUTPUT_DIR, CSV_OUTPUT)\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            \n",
    "            # Convert to list of dicts and upload to Google Sheets\n",
    "            rows = df.to_dict('records')\n",
    "            \n",
    "            # Upload to GG Sheets\n",
    "            # update_sheet(rows)\n",
    "    \n",
    "    print('='*60)\n",
    "    print(f'Saved {subdir}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39fd7e5",
   "metadata": {},
   "source": [
    "# Export to GG Sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00970a0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_cache is only supported with oauth2client<4.0.0 (__init__.py:49)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(18, 9)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os \n",
    "\n",
    "OUTPUT_DIR = 'batch_processing_results'\n",
    "df_list = []\n",
    "\n",
    "# for file in sorted(os.listdir(OUTPUT_DIR)):\n",
    "#     if file.endswith('.csv') and file[0] == '8':\n",
    "#         print(file)\n",
    "file = '8สค68_jpg_extracted_data_20250916214906.csv'\n",
    "df_temp = pd.read_csv(os.path.join(OUTPUT_DIR, file))\n",
    "df_temp = df_temp.fillna(\"\")  # replace NaN with blank\n",
    "df_list.append(df_temp)\n",
    "\n",
    "df = pd.concat(df_list, ignore_index=True)\n",
    "df.sort_values(by=['วันเดือนปี', 'ร้านค้า', 'เลขกำกับ'], inplace=True)\n",
    "rows = df.to_dict('records')\n",
    "\n",
    "update_sheet(rows)\n",
    "\n",
    "df.shape\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
